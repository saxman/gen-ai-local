{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63b14de-21d0-4145-a2da-d2da2af06ff5",
   "metadata": {},
   "source": [
    "# 03.1 - Transformer Models - Query\n",
    "\n",
    "How to load and query a Hugging Face Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a3bee-98cb-4ea5-97eb-d0c5d1008333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade transformers\n",
    "!pip install --quiet --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a187278-6cc8-4020-98e4-35f2ad0122f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": \"auto\",\n",
    "    \"device_map\": \"cuda\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b2105e-4b2d-45cc-b2c6-c18fc9bdc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    **model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28b0149-befe-4567-86a2-719d35c9ee82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : size : 3264 MB\n",
      "model : is quantized : False\n",
      "model : on GPU : True\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.print_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e9f0fd-76e2-45ae-865a-8a262749f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9afd0d-7661-498a-bee3-7919598d2080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "\n",
      "CPU times: user 210 ms, sys: 234 ms, total: 444 ms\n",
      "Wall time: 438 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is the capital of France?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_text,\n",
    "    **generate_kwargs\n",
    ")\n",
    "\n",
    "response = outputs[0][input_text.shape[-1]:]\n",
    "\n",
    "print(tokenizer.decode(response, skip_special_tokens=True) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07b3b74-8cf3-4b16-bbd4-b697e4179134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is a humorous and nonsensical poem that describes a man from Nantucket who wears a large bucket on his head. The man is frustrated that it's not raining, so he decides to throw the bucket away.\n",
      "\n",
      "CPU times: user 404 ms, sys: 786 μs, total: 404 ms\n",
      "Wall time: 403 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Provide a concise summary of the provided text.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "There once was a man from Nantucket,\n",
    "who wore on his head a large bucket.\n",
    "He was thinking one day,\n",
    "that there's no rain today,\n",
    "so he took his bucket then chucked it.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_text,\n",
    "    **generate_kwargs\n",
    ")\n",
    "\n",
    "response = outputs[0][input_text.shape[-1]:]\n",
    "\n",
    "print(tokenizer.decode(response, skip_special_tokens=True) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658f49d-73ff-44ca-8b4a-64336fea9fd1",
   "metadata": {},
   "source": [
    "## Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f442068-2257-498a-95d4-5504cd6d3fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model_id,\n",
    "    **model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048264a6-5479-42de-a6de-ebbfaa885f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the capital of France?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = pipe(messages, max_new_tokens=512)\n",
    "messages = response[0]['generated_text']\n",
    "\n",
    "print(messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6538b4f-f542-4890-8730-6fc88a44cfd6",
   "metadata": {},
   "source": [
    "### Streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91f1e2b-12ee-45ed-af1e-2d6b571cef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True, skl)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer=streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b5d24a-171a-4caf-b90b-1d6285a844e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "\n",
      "Recite the Gettysburg Address\n",
      "\n",
      "assistant\n",
      "\"Four score and seven years ago, our fathers brought forth on this continent a new nation, conceived in liberty and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation or any nation so conceived and so dedicated can long endure. We are met on a great battlefield of that war. We have come to dedicate a portion of that field as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we cannot dedicate, we cannot consecrate, we cannot hallow this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us—that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Recite the Gettysburg Address\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = pipe(messages, max_new_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482857e6-ea52-43be-a0c2-33abdfb5918c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
